{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc49ebcd-832b-40b3-a507-e466e850be68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-1.0.0-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m428.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kagglesdk<1.0,>=0.1.14 (from kagglehub)\n",
      "  Downloading kagglesdk-0.1.15-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from kagglehub) (24.2)\n",
      "Collecting pyyaml (from kagglehub)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from kagglehub) (2.32.5)\n",
      "Collecting tqdm (from kagglehub)\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from kagglesdk<1.0,>=0.1.14->kagglehub) (6.33.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->kagglehub) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->kagglehub) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->kagglehub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->kagglehub) (2025.10.5)\n",
      "Downloading kagglehub-1.0.0-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kagglesdk-0.1.15-py3-none-any.whl (160 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.0/174.0 kB\u001b[0m \u001b[31m229.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, pyyaml, kagglesdk, kagglehub\n",
      "Successfully installed kagglehub-1.0.0 kagglesdk-0.1.15 pyyaml-6.0.3 tqdm-4.67.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'kagglehub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpython3 -m pip install kagglehub\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m path = \u001b[43mkagglehub\u001b[49m.dataset_download(\u001b[33m\"\u001b[39m\u001b[33mandrewmvd/road-sign-detection\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPath to dataset files:\u001b[39m\u001b[33m\"\u001b[39m, path)\n",
      "\u001b[31mNameError\u001b[39m: name 'kagglehub' is not defined"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install kagglehub\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c4b006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading to /Users/admin/.cache/kagglehub/datasets/andrewmvd/road-sign-detection/1.archive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 218M/218M [06:49<00:00, 559kB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/admin/.cache/kagglehub/datasets/andrewmvd/road-sign-detection/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"andrewmvd/road-sign-detection\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58b7ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images', 'annotations']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"/Users/admin/.cache/kagglehub/datasets/andrewmvd/road-sign-detection/versions/1\"\n",
    "\n",
    "print(os.listdir(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e68b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f06ec97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of captions: 877\n",
      "Example captions:\n",
      "['There is 2 speedlimits in the image.', 'There is 2 speedlimits in the image.', 'There are a stop and a trafficlight in the image.', 'There is a speedlimit in the image.', 'There is a speedlimit in the image.']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "\n",
    "BASE_PATH = \"/Users/admin/.cache/kagglehub/datasets/andrewmvd/road-sign-detection/versions/1\"\n",
    "ANNOTATIONS_PATH = os.path.join(BASE_PATH, \"annotations\")\n",
    "\n",
    "def generate_caption_from_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for obj in root.findall(\"object\"):\n",
    "        label = obj.find(\"name\").text\n",
    "        labels.append(label)\n",
    "\n",
    "    if not labels:\n",
    "        return \"No road signs detected.\"\n",
    "\n",
    "    counts = Counter(labels)\n",
    "\n",
    "    phrases = []\n",
    "    for label, count in counts.items():\n",
    "        label = label.lower()\n",
    "        if count == 1:\n",
    "            phrases.append(f\"a {label}\")\n",
    "        else:\n",
    "            phrases.append(f\"{count} {label}s\")\n",
    "\n",
    "    if len(phrases) == 1:\n",
    "        return f\"There is {phrases[0]} in the image.\"\n",
    "    else:\n",
    "        return \"There are \" + \", \".join(phrases[:-1]) + \" and \" + phrases[-1] + \" in the image.\"\n",
    "\n",
    "\n",
    "captions = []\n",
    "image_ids = []\n",
    "\n",
    "for file in os.listdir(ANNOTATIONS_PATH):\n",
    "    if file.endswith(\".xml\"):\n",
    "        xml_path = os.path.join(ANNOTATIONS_PATH, file)\n",
    "        caption = generate_caption_from_xml(xml_path)\n",
    "\n",
    "        image_name = file.replace(\".xml\", \".jpg\")\n",
    "        captions.append(caption)\n",
    "        image_ids.append(image_name)\n",
    "\n",
    "print(\"Number of captions:\", len(captions))\n",
    "print(\"Example captions:\")\n",
    "print(captions[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73fff9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {\n",
    "    \"stop\": \"stop sign\",\n",
    "    \"trafficlight\": \"traffic light\",\n",
    "    \"traffic light\": \"traffic light\",\n",
    "    \"speedlimit\": \"speed limit sign\",\n",
    "    \"crosswalk\": \"crosswalk sign\"\n",
    "}\n",
    "\n",
    "def generate_caption_from_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for obj in root.findall(\"object\"):\n",
    "        raw_label = obj.find(\"name\").text.lower()\n",
    "        label = LABEL_MAP.get(raw_label, raw_label)\n",
    "        labels.append(label)\n",
    "\n",
    "    if not labels:\n",
    "        return \"No road signs detected.\"\n",
    "\n",
    "    counts = Counter(labels)\n",
    "    phrases = []\n",
    "\n",
    "    for label, count in counts.items():\n",
    "        if count == 1:\n",
    "            phrases.append(f\"a {label}\")\n",
    "        else:\n",
    "            phrases.append(f\"{count} {label}s\")\n",
    "\n",
    "    if len(phrases) == 1:\n",
    "        if count == 1:\n",
    "            return f\"There is {phrases[0]} in the image.\"\n",
    "        else:\n",
    "            return f\"There are {phrases[0]} in the image.\"\n",
    "    else:\n",
    "        return \"There are \" + \", \".join(phrases[:-1]) + \" and \" + phrases[-1] + \" in the image.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4af1f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_short_caption(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for obj in root.findall(\"object\"):\n",
    "        raw_label = obj.find(\"name\").text.lower()\n",
    "        label = LABEL_MAP.get(raw_label, raw_label)\n",
    "        labels.append(label)\n",
    "\n",
    "    if not labels:\n",
    "        return \"No road signs detected.\"\n",
    "\n",
    "    counts = Counter(labels)\n",
    "    phrases = []\n",
    "\n",
    "    for label, count in counts.items():\n",
    "        if count == 1:\n",
    "            phrases.append(f\"a {label}\")\n",
    "        else:\n",
    "            phrases.append(f\"{count} {label}s\")\n",
    "\n",
    "    if len(phrases) == 1:\n",
    "        if list(counts.values())[0] == 1:\n",
    "            return f\"There is {phrases[0]} in the image.\"\n",
    "        else:\n",
    "            return f\"There are {phrases[0]} in the image.\"\n",
    "    else:\n",
    "        return \"There are \" + \", \".join(phrases[:-1]) + \" and \" + phrases[-1] + \" in the image.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e0fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "LONG_DESCRIPTIONS = {\n",
    "    \"stop sign\": \"A stop sign is a regulatory traffic sign that instructs drivers to come to a complete stop at an intersection to ensure road safety.\",\n",
    "    \"traffic light\": \"A traffic light is a signaling device positioned at road intersections to control vehicle and pedestrian movement using colored lights.\",\n",
    "    \"speed limit sign\": \"A speed limit sign indicates the maximum legal speed allowed for vehicles on a specific road segment.\",\n",
    "    \"crosswalk sign\": \"A crosswalk sign warns drivers about pedestrian crossing areas and helps improve pedestrian safety.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d49fad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_long_caption(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for obj in root.findall(\"object\"):\n",
    "        raw_label = obj.find(\"name\").text.lower()\n",
    "        label = LABEL_MAP.get(raw_label, raw_label)\n",
    "        labels.append(label)\n",
    "\n",
    "    if not labels:\n",
    "        return \"The image does not contain any visible road signs.\"\n",
    "\n",
    "    counts = Counter(labels)\n",
    "    explanations = []\n",
    "\n",
    "    for label, count in counts.items():\n",
    "        description = LONG_DESCRIPTIONS.get(label, \"\")\n",
    "        if count == 1:\n",
    "            explanations.append(description)\n",
    "        else:\n",
    "            explanations.append(f\"There are {count} instances of {label}s in the image. {description}\")\n",
    "\n",
    "    return \" \".join(explanations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "501073bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of short captions: 877\n",
      "Example short: ['There are 2 speed limit signs in the image.', 'There are 2 speed limit signs in the image.', 'There are a stop sign and a traffic light in the image.']\n",
      "\n",
      "Example long: ['There are 2 instances of speed limit signs in the image. A speed limit sign indicates the maximum legal speed allowed for vehicles on a specific road segment.', 'There are 2 instances of speed limit signs in the image. A speed limit sign indicates the maximum legal speed allowed for vehicles on a specific road segment.']\n"
     ]
    }
   ],
   "source": [
    "short_captions = []\n",
    "long_captions = []\n",
    "\n",
    "for file in os.listdir(ANNOTATIONS_PATH):\n",
    "    if file.endswith(\".xml\"):\n",
    "        xml_path = os.path.join(ANNOTATIONS_PATH, file)\n",
    "\n",
    "        short_cap = generate_short_caption(xml_path)\n",
    "        long_cap = generate_long_caption(xml_path)\n",
    "\n",
    "        short_captions.append(short_cap)\n",
    "        long_captions.append(long_cap)\n",
    "\n",
    "print(\"Number of short captions:\", len(short_captions))\n",
    "print(\"Example short:\", short_captions[:3])\n",
    "print(\"\\nExample long:\", long_captions[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2a27fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 1754\n",
      "['There are 2 speed limit signs in the image.', 'There are 2 speed limit signs in the image.', 'There are a stop sign and a traffic light in the image.']\n"
     ]
    }
   ],
   "source": [
    "all_sentences = short_captions + long_captions\n",
    "\n",
    "print(\"Total sentences:\", len(all_sentences))\n",
    "print(all_sentences[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de53320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 69\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(sentences, min_freq=1):\n",
    "    counter = Counter()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.lower().split()\n",
    "        counter.update(tokens)\n",
    "\n",
    "    vocab = {\n",
    "        \"<PAD>\": 0,\n",
    "        \"<SOS>\": 1,\n",
    "        \"<EOS>\": 2,\n",
    "        \"<UNK>\": 3\n",
    "    }\n",
    "\n",
    "    idx = 4\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(all_sentences)\n",
    "print(\"Vocabulary size:\", len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8ea539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, sentences, vocab, max_len=40):\n",
    "        self.sentences = sentences\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        tokens = sentence.lower().split()\n",
    "\n",
    "        encoded = [self.vocab[\"<SOS>\"]]\n",
    "\n",
    "        for token in tokens:\n",
    "            encoded.append(self.vocab.get(token, self.vocab[\"<UNK>\"]))\n",
    "\n",
    "        encoded.append(self.vocab[\"<EOS>\"])\n",
    "\n",
    "        if len(encoded) < self.max_len:\n",
    "            encoded += [self.vocab[\"<PAD>\"]] * (self.max_len - len(encoded))\n",
    "        else:\n",
    "            encoded = encoded[:self.max_len]\n",
    "\n",
    "        return torch.tensor(encoded)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded = self.encode(self.sentences[idx])\n",
    "        return encoded[:-1], encoded[1:]\n",
    "\n",
    "dataset = CaptionDataset(all_sentences, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b0ac89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CaptionLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        out, _ = self.lstm(embed)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b042fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7918\n",
      "Epoch 2, Loss: 0.2819\n",
      "Epoch 3, Loss: 0.1961\n",
      "Epoch 4, Loss: 0.1817\n",
      "Epoch 5, Loss: 0.1772\n",
      "Epoch 6, Loss: 0.1739\n",
      "Epoch 7, Loss: 0.1710\n",
      "Epoch 8, Loss: 0.1698\n",
      "Epoch 9, Loss: 0.1689\n",
      "Epoch 10, Loss: 0.1674\n",
      "Epoch 11, Loss: 0.1675\n",
      "Epoch 12, Loss: 0.1656\n",
      "Epoch 13, Loss: 0.1655\n",
      "Epoch 14, Loss: 0.1648\n",
      "Epoch 15, Loss: 0.1653\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CaptionLSTM(len(vocab)).to(device)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "\n",
    "        loss = criterion(\n",
    "            output.reshape(-1, len(vocab)),\n",
    "            y.reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b60e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a speed limit sign in the image.\n"
     ]
    }
   ],
   "source": [
    "idx2word = {v:k for k,v in vocab.items()}\n",
    "\n",
    "def generate_text(model, vocab, max_len=40):\n",
    "    model.eval()\n",
    "\n",
    "    input_seq = torch.tensor([[vocab[\"<SOS>\"]]]).to(device)\n",
    "    generated = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        output = model(input_seq)\n",
    "        next_token = output[:, -1, :].argmax(1).item()\n",
    "\n",
    "        if next_token == vocab[\"<EOS>\"]:\n",
    "            break\n",
    "\n",
    "        generated.append(idx2word.get(next_token, \"<UNK>\"))\n",
    "\n",
    "        input_seq = torch.cat(\n",
    "            [input_seq, torch.tensor([[next_token]]).to(device)],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "    return \" \".join(generated)\n",
    "\n",
    "print(generate_text(model, vocab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "619be201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a speed limit sign in the image.\n",
      "there is a speed limit sign in the image.\n",
      "there is a speed limit sign in the image.\n",
      "there is a speed limit sign in the image.\n",
      "there is a speed limit sign in the image.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(generate_text(model, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a5c101b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2 speed limit signs', '2 speed limit signs', 'a stop sign and a traffic light']\n",
      "['there are 2 speed limit signs in the image.', 'there are 2 speed limit signs in the image.', 'there are a stop sign and a traffic light in the image.']\n"
     ]
    }
   ],
   "source": [
    "training_inputs = []\n",
    "training_targets = []\n",
    "\n",
    "for sentence in short_captions:\n",
    "    clean = sentence.lower()\n",
    "    \n",
    "    # Extract label phrase\n",
    "    label_part = clean.replace(\"there is \", \"\") \\\n",
    "                      .replace(\"there are \", \"\") \\\n",
    "                      .replace(\" in the image.\", \"\")\n",
    "    \n",
    "    training_inputs.append(label_part)\n",
    "    training_targets.append(clean)\n",
    "\n",
    "print(training_inputs[:3])\n",
    "print(training_targets[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ed696a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 28\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(sentences, min_freq=1):\n",
    "    counter = Counter()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split()\n",
    "        counter.update(tokens)\n",
    "\n",
    "    vocab = {\n",
    "        \"<PAD>\": 0,\n",
    "        \"<SOS>\": 1,\n",
    "        \"<EOS>\": 2,\n",
    "        \"<UNK>\": 3\n",
    "    }\n",
    "\n",
    "    idx = 4\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "    return vocab\n",
    "\n",
    "all_text = training_inputs + training_targets\n",
    "vocab = build_vocab(all_text)\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3335f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ConditionalCaptionDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, vocab, max_len=30):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        tokens = sentence.split()\n",
    "        encoded = [self.vocab[\"<SOS>\"]]\n",
    "\n",
    "        for token in tokens:\n",
    "            encoded.append(self.vocab.get(token, self.vocab[\"<UNK>\"]))\n",
    "\n",
    "        encoded.append(self.vocab[\"<EOS>\"])\n",
    "\n",
    "        if len(encoded) < self.max_len:\n",
    "            encoded += [self.vocab[\"<PAD>\"]] * (self.max_len - len(encoded))\n",
    "        else:\n",
    "            encoded = encoded[:self.max_len]\n",
    "\n",
    "        return torch.tensor(encoded)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_encoded = self.encode(self.inputs[idx])\n",
    "        target_encoded = self.encode(self.targets[idx])\n",
    "\n",
    "        return input_encoded, target_encoded\n",
    "\n",
    "dataset = ConditionalCaptionDataset(training_inputs, training_targets, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1914fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Seq2SeqCaptionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq, target_seq):\n",
    "        # Encode input\n",
    "        embedded_input = self.embedding(input_seq)\n",
    "        _, (hidden, cell) = self.encoder(embedded_input)\n",
    "\n",
    "        # Decode target\n",
    "        embedded_target = self.embedding(target_seq)\n",
    "        outputs, _ = self.decoder(embedded_target, (hidden, cell))\n",
    "\n",
    "        outputs = self.fc(outputs)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a23163f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.4784\n",
      "Epoch 2, Loss: 0.3632\n",
      "Epoch 3, Loss: 0.2553\n",
      "Epoch 4, Loss: 0.2217\n",
      "Epoch 5, Loss: 0.2098\n",
      "Epoch 6, Loss: 0.2034\n",
      "Epoch 7, Loss: 0.1822\n",
      "Epoch 8, Loss: 0.1380\n",
      "Epoch 9, Loss: 0.1134\n",
      "Epoch 10, Loss: 0.0799\n",
      "Epoch 11, Loss: 0.0668\n",
      "Epoch 12, Loss: 0.0510\n",
      "Epoch 13, Loss: 0.0406\n",
      "Epoch 14, Loss: 0.0357\n",
      "Epoch 15, Loss: 0.0284\n",
      "Epoch 16, Loss: 0.0241\n",
      "Epoch 17, Loss: 0.0224\n",
      "Epoch 18, Loss: 0.0177\n",
      "Epoch 19, Loss: 0.0165\n",
      "Epoch 20, Loss: 0.0138\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Seq2SeqCaptionModel(len(vocab)).to(device)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    for input_seq, target_seq in loader:\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input_seq, target_seq[:, :-1])\n",
    "\n",
    "        loss = criterion(\n",
    "            output.reshape(-1, len(vocab)),\n",
    "            target_seq[:, 1:].reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bca2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {v:k for k,v in vocab.items()}\n",
    "\n",
    "def generate_caption(model, vocab, input_text, max_len=30):\n",
    "    model.eval()\n",
    "\n",
    "    tokens = input_text.split()\n",
    "    encoded_input = [vocab[\"<SOS>\"]]\n",
    "\n",
    "    for token in tokens:\n",
    "        encoded_input.append(vocab.get(token, vocab[\"<UNK>\"]))\n",
    "\n",
    "    encoded_input.append(vocab[\"<EOS>\"])\n",
    "\n",
    "    input_tensor = torch.tensor([encoded_input]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedded_input = model.embedding(input_tensor)\n",
    "        _, (hidden, cell) = model.encoder(embedded_input)\n",
    "\n",
    "    decoder_input = torch.tensor([[vocab[\"<SOS>\"]]]).to(device)\n",
    "\n",
    "    generated = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        embedded = model.embedding(decoder_input)\n",
    "        output, (hidden, cell) = model.decoder(embedded, (hidden, cell))\n",
    "        prediction = model.fc(output[:, -1, :])\n",
    "\n",
    "        next_token = prediction.argmax(1).item()\n",
    "\n",
    "        if next_token == vocab[\"<EOS>\"]:\n",
    "            break\n",
    "\n",
    "        generated.append(idx2word.get(next_token, \"<UNK>\"))\n",
    "\n",
    "        decoder_input = torch.tensor([[next_token]]).to(device)\n",
    "\n",
    "    return \" \".join(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fe2f4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a crosswalk sign in the image.\n",
      "speed limit sign in the image.\n",
      "there are a traffic light in the image.\n"
     ]
    }
   ],
   "source": [
    "print(generate_caption(model, vocab, \"a stop sign\"))\n",
    "print(generate_caption(model, vocab, \"2 speed limit signs\"))\n",
    "print(generate_caption(model, vocab, \"a traffic light\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1aea12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption_beam(model, vocab, input_text, beam_width=3, max_len=30):\n",
    "    model.eval()\n",
    "    idx2word = {v:k for k,v in vocab.items()}\n",
    "\n",
    "    tokens = input_text.split()\n",
    "    encoded_input = [vocab[\"<SOS>\"]]\n",
    "    for token in tokens:\n",
    "        encoded_input.append(vocab.get(token, vocab[\"<UNK>\"]))\n",
    "    encoded_input.append(vocab[\"<EOS>\"])\n",
    "\n",
    "    input_tensor = torch.tensor([encoded_input]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedded_input = model.embedding(input_tensor)\n",
    "        _, (hidden, cell) = model.encoder(embedded_input)\n",
    "\n",
    "    sequences = [[ [vocab[\"<SOS>\"]], 0.0, hidden, cell ]]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score, h, c in sequences:\n",
    "            decoder_input = torch.tensor([[seq[-1]]]).to(device)\n",
    "            embedded = model.embedding(decoder_input)\n",
    "            output, (h_new, c_new) = model.decoder(embedded, (h, c))\n",
    "            prediction = model.fc(output[:, -1, :])\n",
    "\n",
    "            log_probs = torch.log_softmax(prediction, dim=1)\n",
    "            topk = torch.topk(log_probs, beam_width)\n",
    "\n",
    "            for i in range(beam_width):\n",
    "                token = topk.indices[0][i].item()\n",
    "                candidate = [\n",
    "                    seq + [token],\n",
    "                    score - topk.values[0][i].item(),\n",
    "                    h_new,\n",
    "                    c_new\n",
    "                ]\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        sequences = sorted(all_candidates, key=lambda x: x[1])[:beam_width]\n",
    "\n",
    "    best_seq = sequences[0][0]\n",
    "\n",
    "    words = []\n",
    "    for token in best_seq:\n",
    "        if token == vocab[\"<EOS>\"]:\n",
    "            break\n",
    "        if token not in [vocab[\"<SOS>\"], vocab[\"<PAD>\"]]:\n",
    "            words.append(idx2word.get(token, \"\"))\n",
    "\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc298fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a crosswalk sign in the image.\n",
      "speed limit sign in the image.\n",
      "there is a traffic light in the image.\n"
     ]
    }
   ],
   "source": [
    "print(generate_caption_beam(model, vocab, \"a stop sign\"))\n",
    "print(generate_caption_beam(model, vocab, \"2 speed limit signs\"))\n",
    "print(generate_caption_beam(model, vocab, \"a traffic light\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88c8bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption_stable(model, vocab, input_text, max_len=30):\n",
    "    model.eval()\n",
    "    idx2word = {v:k for k,v in vocab.items()}\n",
    "\n",
    "    tokens = input_text.split()\n",
    "    encoded_input = [vocab[\"<SOS>\"]]\n",
    "\n",
    "    for token in tokens:\n",
    "        encoded_input.append(vocab.get(token, vocab[\"<UNK>\"]))\n",
    "\n",
    "    encoded_input.append(vocab[\"<EOS>\"])\n",
    "    input_tensor = torch.tensor([encoded_input]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedded_input = model.embedding(input_tensor)\n",
    "        _, (hidden, cell) = model.encoder(embedded_input)\n",
    "\n",
    "    decoder_input = torch.tensor([[vocab[\"<SOS>\"]]]).to(device)\n",
    "    generated = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        embedded = model.embedding(decoder_input)\n",
    "        output, (hidden, cell) = model.decoder(embedded, (hidden, cell))\n",
    "        prediction = model.fc(output[:, -1, :])\n",
    "\n",
    "        next_token = prediction.argmax(1).item()\n",
    "\n",
    "        if next_token == vocab[\"<EOS>\"]:\n",
    "            break\n",
    "\n",
    "        word = idx2word.get(next_token, \"\")\n",
    "        generated.append(word)\n",
    "\n",
    "        decoder_input = torch.tensor([[next_token]]).to(device)\n",
    "\n",
    "    sentence = \" \".join(generated)\n",
    "\n",
    "    # ---- Grammar Stabilization ----\n",
    "    if input_text.strip().startswith(\"a \"):\n",
    "        if not sentence.startswith(\"there is\"):\n",
    "            sentence = \"there is \" + input_text + \" in the image.\"\n",
    "    elif input_text.strip()[0].isdigit():\n",
    "        if not sentence.startswith(\"there are\"):\n",
    "            sentence = \"there are \" + input_text + \" in the image.\"\n",
    "\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91e5cb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a crosswalk sign in the image.\n",
      "there are 2 speed limit signs in the image.\n",
      "there is a traffic light in the image.\n"
     ]
    }
   ],
   "source": [
    "print(generate_caption_stable(model, vocab, \"a stop sign\"))\n",
    "print(generate_caption_stable(model, vocab, \"2 speed limit signs\"))\n",
    "print(generate_caption_stable(model, vocab, \"a traffic light\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a1dc382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Accuracy: 0.7947548460661346\n"
     ]
    }
   ],
   "source": [
    "def evaluate_exact_match(model, dataset, vocab):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    for input_text, target_text in zip(training_inputs, training_targets):\n",
    "        generated = generate_caption_stable(model, vocab, input_text)\n",
    "        if generated.strip() == target_text.strip():\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / len(training_inputs)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_exact_match(model, dataset, vocab)\n",
    "print(\"Exact Match Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bab0b92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Accuracy: 0.9965266471290568\n"
     ]
    }
   ],
   "source": [
    "def token_accuracy(model, dataset, vocab):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "    for input_seq, target_seq in loader:\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "\n",
    "        output = model(input_seq, target_seq[:, :-1])\n",
    "        predictions = output.argmax(2)\n",
    "\n",
    "        for i in range(target_seq.shape[1]-1):\n",
    "            if target_seq[0, i+1] != vocab[\"<PAD>\"]:\n",
    "                total += 1\n",
    "                if predictions[0, i] == target_seq[0, i+1]:\n",
    "                    correct += 1\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "print(\"Token Accuracy:\", token_accuracy(model, dataset, vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2121b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(\n",
    "    training_inputs,\n",
    "    training_targets,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18162068",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ConditionalCaptionDataset(train_inputs, train_targets, vocab)\n",
    "val_dataset   = ConditionalCaptionDataset(val_inputs, val_targets, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48c92661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 0.0125\n",
      "Val Loss:   0.0119\n",
      "------------------------------\n",
      "Epoch 2\n",
      "Train Loss: 0.0100\n",
      "Val Loss:   0.0092\n",
      "------------------------------\n",
      "Epoch 3\n",
      "Train Loss: 0.0087\n",
      "Val Loss:   0.0109\n",
      "------------------------------\n",
      "Epoch 4\n",
      "Train Loss: 0.0085\n",
      "Val Loss:   0.0096\n",
      "------------------------------\n",
      "Epoch 5\n",
      "Train Loss: 0.0070\n",
      "Val Loss:   0.0086\n",
      "------------------------------\n",
      "Epoch 6\n",
      "Train Loss: 0.0066\n",
      "Val Loss:   0.0090\n",
      "------------------------------\n",
      "Epoch 7\n",
      "Train Loss: 0.0066\n",
      "Val Loss:   0.0080\n",
      "------------------------------\n",
      "Epoch 8\n",
      "Train Loss: 0.0063\n",
      "Val Loss:   0.0077\n",
      "------------------------------\n",
      "Epoch 9\n",
      "Train Loss: 0.0114\n",
      "Val Loss:   0.0194\n",
      "------------------------------\n",
      "Epoch 10\n",
      "Train Loss: 0.0120\n",
      "Val Loss:   0.0099\n",
      "------------------------------\n",
      "Epoch 11\n",
      "Train Loss: 0.0072\n",
      "Val Loss:   0.0087\n",
      "------------------------------\n",
      "Epoch 12\n",
      "Train Loss: 0.0075\n",
      "Val Loss:   0.0086\n",
      "------------------------------\n",
      "Epoch 13\n",
      "Train Loss: 0.0058\n",
      "Val Loss:   0.0101\n",
      "------------------------------\n",
      "Epoch 14\n",
      "Train Loss: 0.0047\n",
      "Val Loss:   0.0068\n",
      "------------------------------\n",
      "Epoch 15\n",
      "Train Loss: 0.0033\n",
      "Val Loss:   0.0066\n",
      "------------------------------\n",
      "Epoch 16\n",
      "Train Loss: 0.0026\n",
      "Val Loss:   0.0067\n",
      "------------------------------\n",
      "Epoch 17\n",
      "Train Loss: 0.0023\n",
      "Val Loss:   0.0065\n",
      "------------------------------\n",
      "Epoch 18\n",
      "Train Loss: 0.0021\n",
      "Val Loss:   0.0065\n",
      "------------------------------\n",
      "Epoch 19\n",
      "Train Loss: 0.0018\n",
      "Val Loss:   0.0064\n",
      "------------------------------\n",
      "Epoch 20\n",
      "Train Loss: 0.0016\n",
      "Val Loss:   0.0062\n",
      "------------------------------\n",
      "Epoch 21\n",
      "Train Loss: 0.0015\n",
      "Val Loss:   0.0062\n",
      "------------------------------\n",
      "Epoch 22\n",
      "Train Loss: 0.0014\n",
      "Val Loss:   0.0064\n",
      "------------------------------\n",
      "Epoch 23\n",
      "Train Loss: 0.0013\n",
      "Val Loss:   0.0061\n",
      "------------------------------\n",
      "Epoch 24\n",
      "Train Loss: 0.0012\n",
      "Val Loss:   0.0062\n",
      "------------------------------\n",
      "Epoch 25\n",
      "Train Loss: 0.0012\n",
      "Val Loss:   0.0062\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for input_seq, target_seq in train_loader:\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input_seq, target_seq[:, :-1])\n",
    "\n",
    "        loss = criterion(\n",
    "            output.reshape(-1, len(vocab)),\n",
    "            target_seq[:, 1:].reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq in val_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq, target_seq[:, :-1])\n",
    "\n",
    "            loss = criterion(\n",
    "                output.reshape(-1, len(vocab)),\n",
    "                target_seq[:, 1:].reshape(-1)\n",
    "            )\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "    print(f\"Val Loss:   {val_loss/len(val_loader):.4f}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f33789e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab': vocab\n",
    "}, \"nlp_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
